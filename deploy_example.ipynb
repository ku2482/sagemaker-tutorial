{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "from io import StringIO\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test at local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from node import node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './source_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer.\n",
    "with open(os.path.join(model_dir, 'modules.pickle'), 'rb') as f:\n",
    "    modules = pickle.load(f)\n",
    "vectorizer = SentenceTransformer(modules=modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tree.\n",
    "with open(os.path.join(model_dir, 'tree.pkl'), 'rb') as f:\n",
    "    tree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text.\n",
    "text = ['It still looks brand new too!! Really great!!']\n",
    "embeddings = vectorizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search items.\n",
    "response = tree.binary_search(embeddings[0])\n",
    "response = {f'result{str(i)}': res for i, res in enumerate(response)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- result0\n",
      "{'review_id': 'R14A9UA4963BHV', 'product_id': 'B00N762OVC', 'review_body': 'Took to Universal Studios and they work well at both parks for the Harry Potter Interactive areas.  These are the same ones you can buy at Universal Studios.', 'sentence': 'these are the same ones you can buy at universal studios'}\n",
      "- result1\n",
      "{'review_id': 'RSMVI4CNQFOQ', 'product_id': 'B00RJNM9Q4', 'review_body': 'Took to Universal Studios and they work well at both parks for the Harry Potter Interactive areas.  These are the same ones you can buy at Universal Studios.', 'sentence': 'these are the same ones you can buy at universal studios'}\n",
      "- result2\n",
      "{'review_id': 'R1LIZTMDTFHQWJ', 'product_id': 'B00XTRG5UA', 'review_body': 'It was exactly what I expected! Received it and its in mint condition and is brand new! Very happy!', 'sentence': 'received it and its in mint condition and is brand new'}\n"
     ]
    }
   ],
   "source": [
    "for k, r in response.items():\n",
    "    print(f'- {k}')\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator.\n",
    "estimator = PyTorch(\n",
    "    entry_point='entry_point.py',\n",
    "    source_dir='source_dir',\n",
    "    dependencies=['node'],\n",
    "    role=role,\n",
    "    framework_version='1.3.1',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-11 09:59:44 Starting - Starting the training job...\n",
      "2020-01-11 09:59:45 Starting - Launching requested ML instances.........\n",
      "2020-01-11 10:01:15 Starting - Preparing the instances for training......\n",
      "2020-01-11 10:02:23 Downloading - Downloading input data...\n",
      "2020-01-11 10:02:58 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:25,652 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:25,655 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:25,669 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:25,886 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2020-01-11 10:03:24 Training - Training image download completed. Training in progress.\u001b[34m2020-01-11 10:03:35,637 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:35,637 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:35,638 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-11 10:03:35,638 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpbsqmx_jn/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.40.0)\u001b[0m\n",
      "\u001b[34mCollecting Unidecode\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/63/cd93f1cdaf93c3e4aa1827373492d74de5c58af3613ba1eb3dd5c01f4128/nvidia-ml-py-375.53.tar.gz\u001b[0m\n",
      "\u001b[34mCollecting clean-text\n",
      "  Downloading https://files.pythonhosted.org/packages/23/98/2650271bc1052002ad7e61595f7a44ff24f6bb4eb24d9c0e42e92c991708/clean_text-0.1.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting sentence-transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\u001b[0m\n",
      "\u001b[34mCollecting ftfy\n",
      "  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (0.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 5)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->clean-text->-r requirements.txt (line 4)) (0.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.10.32)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/64/1b0eb918ebdfba27b4c148853ed93cc38d83aa452882f2a9dc64acaa9b2f/regex-2020.1.8-cp36-cp36m-manylinux2010_x86_64.whl (689kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 5)) (0.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.14.0,>=1.13.32 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.13.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<2.8.1,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers==2.3.0->sentence-transformers->-r requirements.txt (line 5)) (2.8.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nvidia-ml-py, sentence-transformers, default-user-module-name, ftfy, nltk, sacremoses\n",
      "  Building wheel for nvidia-ml-py (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nvidia-ml-py (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py: filename=nvidia_ml_py-375.53.1-cp36-none-any.whl size=22387 sha256=571b47c845c8d87d5c00fd1ab47382085065681513f38989739e96a1bb3ad07c\n",
      "  Stored in directory: /root/.cache/pip/wheels/8f/92/ec/eca76ea27bf7699c68c1fb6c95fdadbbf64addf2c62bb72b20\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-cp36-none-any.whl size=64943 sha256=9dcf2af82d55dc4eb970891fda2c710e7cd03c4593bfd3b4bf746097062fcb7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=463768937 sha256=c536cd845f0c43be8e141c67654ed93421ce41cbb2b74f40e2251b052287ffe4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8yibu_e9/wheels/ba/5e/9c/c5c1da1cac2e757ec4a4810ec1902ce348b17a6f4f155566ec\u001b[0m\n",
      "\u001b[34m  Building wheel for ftfy (setup.py): started\n",
      "  Building wheel for ftfy (setup.py): finished with status 'done'\n",
      "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=ba8659c85bd02bcb42118b098a28918de200e2e17d61b6099049d1a401fb1259\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=9d126441e0549bbfc69d732f94bae0fa52a72874b32b3bfb443c72d1bb38dfde\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=9804be3e6bae63aedc7627f5626d671afc2ec95b3d31d0f2bae8997175f0a3be\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\u001b[0m\n",
      "\u001b[34mSuccessfully built nvidia-ml-py sentence-transformers default-user-module-name ftfy nltk sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: Unidecode, nvidia-ml-py, ftfy, clean-text, regex, sentencepiece, sacremoses, transformers, nltk, sentence-transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed Unidecode-1.1.1 clean-text-0.1.1 default-user-module-name-1.0.0 ftfy-5.6 nltk-3.4.5 nvidia-ml-py-375.53.1 regex-2020.1.8 sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\u001b[0m\n",
      "\u001b[34m2020-01-11 10:04:32,897 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-11 10:04:32,915 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-11 10:04:32,932 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-11 10:04:32,947 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-01-11-09-58-57-012\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-11-09-58-57-012/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-11-09-58-57-012/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-11-09-58-57-012\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-010942746803/pytorch-training-2020-01-11-09-58-57-012/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python entry_point.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34m2020-01-11 10:04:40,125 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-11 10:04:47 Uploading - Uploading generated training model\n",
      "2020-01-11 10:06:04 Completed - Training job completed\n",
      "Training seconds: 221\n",
      "Billable seconds: 221\n"
     ]
    }
   ],
   "source": [
    "# Train.\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model.\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    'query': 'it still looks brand new too',\n",
    "    'n_items': 10}\n",
    "response = predictor.predict(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "- review_id\n",
      "R2JFFVNUQIFY7J\n",
      "- product_id\n",
      "B00DSIQS6G\n",
      "- review_body\n",
      "My son loves closing these; he's just learning how they open. The toy is very solid and stands up to his using his feet on it, standing on it, throwing it around, etc. I don't know how long he'll continue to play with it but for now it gets a lot of attention from my 1 year old.\n",
      "- sentence\n",
      "the toy is very solid and stands up to his using his feet on it, standing on it, throwing it around, etc\n",
      "----------\n",
      "- review_id\n",
      "R2YJOKZYDR789Z\n",
      "- product_id\n",
      "B00FIX22YQ\n",
      "- review_body\n",
      "This is quality wooden product. My 2 years old love playing with them. At this stage, she likes to sort all the same color or same shape. really recommend.\n",
      "- sentence\n",
      "at this stage, she likes to sort all the same color or same shape\n",
      "----------\n",
      "- review_id\n",
      "RJTE957HDA1SQ\n",
      "- product_id\n",
      "B00DJ49AHI\n",
      "- review_body\n",
      "Great dress.  Has been washed multiple times, and still remains intact. My niece absolutely adores it and I think she actually wore it on a daily basis for at least a month.\n",
      "- sentence\n",
      "has been washed multiple times, and still remains intact\n",
      "----------\n",
      "- review_id\n",
      "RAULGDO331VOP\n",
      "- product_id\n",
      "B0032W1LW4\n",
      "- review_body\n",
      "This puzzle is very colorful and challenging. It took two seven year olds and two five years old, plus grandpa two days to put together.  Kept them busy and entertained.  Pieces are solid enough to last several put togethers and put aways without bending.\n",
      "- sentence\n",
      "pieces are solid enough to last several put togethers and put aways without bending\n"
     ]
    }
   ],
   "source": [
    "for value in response.values():\n",
    "    print('-' * 10)\n",
    "    for k, v in value.items():\n",
    "        print(f'- {k}')\n",
    "        print(f'{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
